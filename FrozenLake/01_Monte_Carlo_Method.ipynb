{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f6c45c6",
   "metadata": {},
   "source": [
    "# FrozenLake-v1 환경을 통한 Monte Carlo Method 실습\n",
    "\n",
    "# ToDo: MC에 대한 설명 추가\n",
    "- sampling을 통해 추정(approximate)한다.\n",
    "- 수식 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be55361",
   "metadata": {},
   "source": [
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8403e118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76afcc6",
   "metadata": {},
   "source": [
    "**MC 에서는 episode가 종단 상태를 만나 끝에 도달해야 한다.**\n",
    "\n",
    "FrozenLake 에서는 Goal에 도착하면 1의 reward를 얻고, 나머지 상태에서는 0의 reward를 얻는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29eb7bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f251e771",
   "metadata": {},
   "source": [
    "## Generate episode (sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "277253e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, policy):\n",
    "    states, actions, rewards = [], [], []\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    while True:\n",
    "        # Append State\n",
    "        states.append(state)\n",
    "        \n",
    "        # Append Action\n",
    "        probs = policy[state]\n",
    "        action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "        actions.append(action)\n",
    "        \n",
    "        state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Append reward\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c415f3fc",
   "metadata": {},
   "source": [
    "**몇번의 episode가 있어야 goal에 도달하는 episode를 얻을 수 있을까?**\n",
    "\n",
    "- policy probability가 동일한 random policy에서 시뮬레이션해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62269b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 84\n",
      "states: [0, 0, 1, 2, 6, 10, 14]\n",
      "actions: [0, 1, 1, 1, 2, 2, 3]\n",
      "rewards: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=20)\n",
    "\n",
    "policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "\n",
    "step = 0\n",
    "while True:\n",
    "    step += 1\n",
    "    states, actions, rewards = generate_episode(env, policy)\n",
    "    \n",
    "    if rewards[-1] == 1.0:\n",
    "        break\n",
    "    \n",
    "print(\"step:\", step)\n",
    "print('states:', states)\n",
    "print('actions:', actions)\n",
    "print('rewards:', rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a6e328",
   "metadata": {},
   "source": [
    "# Monte Carlo predictino for Value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8a71c6",
   "metadata": {},
   "source": [
    "## Every-visit MC prediction\n",
    "\n",
    "각 episode에서 마주치는 모든 state에 대해 state value function을 update한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffa497de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def every_visit_MC_prediction(env, policy, n_sample, gamma = 1.0):\n",
    "    \n",
    "    # 특정 state를 방문한 횟수\n",
    "    N = np.zeros(env.nS)\n",
    "    \n",
    "    # state value function\n",
    "    V = np.zeros(env.nS)\n",
    "    \n",
    "    for i in range(n_sample):\n",
    "        states, actions, rewards = generate_episode(env, policy)\n",
    "        \n",
    "        G = 0\n",
    "        \n",
    "        for t in range(len(states) -1, -1, -1):\n",
    "            S = states[t]\n",
    "            G = gamma * G + rewards[t]\n",
    "            N[S] += 1\n",
    "            V[S] = V[S] + (G - V[S]) / N[S]\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53d4a195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Value function\n",
      "[0.00458278 0.00419146 0.00898969 0.00363001 0.00684434 0.\n",
      " 0.02625331 0.         0.01960489 0.05973598 0.11115034 0.\n",
      " 0.         0.1330648  0.38556069 0.        ]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=30)\n",
    "\n",
    "# sample의 갯수\n",
    "n_sample = 50000\n",
    "\n",
    "# random policy\n",
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "\n",
    "every_visit_Value_function = every_visit_MC_prediction(env, random_policy, n_sample, 0.9)\n",
    "\n",
    "print('State Value function')\n",
    "print(every_visit_Value_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f313d19",
   "metadata": {},
   "source": [
    "## First-visit MC prediction\n",
    "\n",
    "각 episode를 통해 backprop update 중에 마주치는 각 state는 한번씩만 update된다. (중복된 state는 update하지 않는다.)\n",
    "\n",
    "- 구현하는 방법은 여러가지가 있을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b1a36d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_MC_prediction(env, policy, n_sample, gamma = 1.0):\n",
    "    \n",
    "    N = np.zeros(env.nS)\n",
    "    V = np.zeros(env.nS)\n",
    "    visit = np.zeros(env.nS, dtype=int) - 1\n",
    "    \n",
    "    for i in range(n_sample):\n",
    "        states, actions, rewards = generate_episode(env, policy)\n",
    "        \n",
    "        G = 0\n",
    "        \n",
    "        for t in range(len(states) - 1, -1, -1):\n",
    "            S = states[t]\n",
    "            G = gamma * G + rewards[t]\n",
    "            \n",
    "            if visit[S] != i:\n",
    "                visit[S] = i\n",
    "                N[S] += 1\n",
    "                V[S] = V[S] + (G - V[S]) / N[S]\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "871f04c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Value function\n",
      "[0.00572872 0.00471985 0.01131949 0.00329021 0.00779079 0.\n",
      " 0.02619256 0.         0.02077171 0.06251912 0.11460876 0.\n",
      " 0.         0.14114219 0.43596378 0.        ]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=30)\n",
    "\n",
    "# sample의 갯수\n",
    "n_sample = 50000\n",
    "\n",
    "# random policy\n",
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "\n",
    "first_visit_Value_function = first_visit_MC_prediction(env, random_policy, n_sample, 0.9)\n",
    "\n",
    "print('State Value function')\n",
    "print(first_visit_Value_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1361e4",
   "metadata": {},
   "source": [
    "### 두 방법의 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1c1113c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0046 0.0057\n",
      "0.0042 0.0047\n",
      "0.0090 0.0113\n",
      "0.0036 0.0033\n",
      "0.0068 0.0078\n",
      "0.0000 0.0000\n",
      "0.0263 0.0262\n",
      "0.0000 0.0000\n",
      "0.0196 0.0208\n",
      "0.0597 0.0625\n",
      "0.1112 0.1146\n",
      "0.0000 0.0000\n",
      "0.0000 0.0000\n",
      "0.1331 0.1411\n",
      "0.3856 0.4360\n",
      "0.0000 0.0000\n"
     ]
    }
   ],
   "source": [
    "for i in range(env.nS):\n",
    "    print('{:.4f} {:.4f}'.format(every_visit_Value_function[i], first_visit_Value_function[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e492900b",
   "metadata": {},
   "source": [
    "# Monte Carlo predictino for Q-function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90027df",
   "metadata": {},
   "source": [
    "## Every-visit MC prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "148edc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def every_visit_MC_Q_prediction(env, policy, n_sample, gamma = 1.0):\n",
    "    N = np.zeros([env.nS, env.nA])\n",
    "    Q = np.zeros([env.nS, env.nA])\n",
    "    \n",
    "    for i in range(n_sample):\n",
    "        states, actions, rewards = generate_episode(env, policy)\n",
    "        \n",
    "        G = 0\n",
    "        \n",
    "        for t in range(len(states)-1, -1, -1):\n",
    "            S = states[t]\n",
    "            A = actions[t]\n",
    "            G = gamma * G + rewards[t]\n",
    "            \n",
    "            N[S, A] += 1\n",
    "            Q[S, A] = Q[S, A] + (G - Q[S, A]) / N[S, A]\n",
    "            \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eaa79c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Value function\n",
      "[[0.00487364 0.00483908 0.00453006 0.00350828]\n",
      " [0.00219985 0.00415718 0.00450523 0.00555318]\n",
      " [0.01294797 0.01013901 0.01071531 0.00530159]\n",
      " [0.00361115 0.00447873 0.00251629 0.00548525]\n",
      " [0.00945723 0.00710523 0.00766139 0.00348851]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.0309953  0.0344699  0.03604571 0.00157178]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.00741656 0.02422018 0.02003064 0.02589799]\n",
      " [0.03993293 0.08026032 0.0735515  0.0373133 ]\n",
      " [0.1305101  0.13630822 0.15018893 0.02636338]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.04343365 0.15288292 0.16409099 0.12050047]\n",
      " [0.19681495 0.5060536  0.50226863 0.40113193]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=30)\n",
    "\n",
    "# sample의 갯수\n",
    "n_sample = 50000\n",
    "\n",
    "# random policy\n",
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "\n",
    "every_visit_Q = every_visit_MC_Q_prediction(env, random_policy, n_sample, 0.9)\n",
    "\n",
    "print('Action Value function')\n",
    "print(every_visit_Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df320f5",
   "metadata": {},
   "source": [
    "## First-visit MC prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e63a3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_visit_MC_Q_prediction(env, policy, n_sample, gamma = 1.0):\n",
    "    N = np.zeros([env.nS, env.nA])\n",
    "    Q = np.zeros([env.nS, env.nA])\n",
    "    visit = np.zeros([env.nS, env.nA], dtype='int') - 1\n",
    "    for i in range(n_sample):\n",
    "        states, actions, rewards = generate_episode(env, policy)\n",
    "        \n",
    "        G = 0\n",
    "        \n",
    "        for t in range(len(states)-1, -1, -1):\n",
    "            S = states[t]\n",
    "            A = actions[t]\n",
    "            G = gamma * G + rewards[t]\n",
    "            \n",
    "            if visit[S, A] != i:\n",
    "                visit[S, A] = i\n",
    "                N[S, A] += 1\n",
    "                Q[S, A] = Q[S, A] + (G - Q[S, A]) / N[S, A]\n",
    "            \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a5a2f06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Value function\n",
      "[[0.00506254 0.00544273 0.00465041 0.00421077]\n",
      " [0.0028829  0.00409655 0.00412922 0.00550699]\n",
      " [0.01117867 0.0092762  0.01357478 0.00458708]\n",
      " [0.00410235 0.00363861 0.00280675 0.00620961]\n",
      " [0.00877252 0.00809771 0.00675081 0.00335377]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.0301989  0.02706705 0.0360676  0.00292674]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.0089488  0.02178521 0.02230243 0.02507339]\n",
      " [0.04756612 0.07964776 0.0708331  0.04238941]\n",
      " [0.14927881 0.12141953 0.1238912  0.01923141]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05543773 0.17754105 0.17366302 0.12896567]\n",
      " [0.20935614 0.50253496 0.51651626 0.42278114]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "env = gym.wrappers.TimeLimit(env, max_episode_steps=30)\n",
    "\n",
    "# sample의 갯수\n",
    "n_sample = 50000\n",
    "\n",
    "# random policy\n",
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "\n",
    "first_visit_Q = first_visit_MC_Q_prediction(env, random_policy, n_sample, 0.9)\n",
    "\n",
    "print('Action Value function')\n",
    "print(first_visit_Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495ee5d6",
   "metadata": {},
   "source": [
    "### 두 방법의 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad2c060c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0049 0.0048 0.0045 0.0035]\n",
      "[0.0051 0.0054 0.0047 0.0042]\n",
      "\n",
      "[0.0022 0.0042 0.0045 0.0056]\n",
      "[0.0029 0.0041 0.0041 0.0055]\n",
      "\n",
      "[0.0129 0.0101 0.0107 0.0053]\n",
      "[0.0112 0.0093 0.0136 0.0046]\n",
      "\n",
      "[0.0036 0.0045 0.0025 0.0055]\n",
      "[0.0041 0.0036 0.0028 0.0062]\n",
      "\n",
      "[0.0095 0.0071 0.0077 0.0035]\n",
      "[0.0088 0.0081 0.0068 0.0034]\n",
      "\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "\n",
      "[0.031  0.0345 0.036  0.0016]\n",
      "[0.0302 0.0271 0.0361 0.0029]\n",
      "\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "\n",
      "[0.0074 0.0242 0.02   0.0259]\n",
      "[0.0089 0.0218 0.0223 0.0251]\n",
      "\n",
      "[0.0399 0.0803 0.0736 0.0373]\n",
      "[0.0476 0.0796 0.0708 0.0424]\n",
      "\n",
      "[0.1305 0.1363 0.1502 0.0264]\n",
      "[0.1493 0.1214 0.1239 0.0192]\n",
      "\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "\n",
      "[0.0434 0.1529 0.1641 0.1205]\n",
      "[0.0554 0.1775 0.1737 0.129 ]\n",
      "\n",
      "[0.1968 0.5061 0.5023 0.4011]\n",
      "[0.2094 0.5025 0.5165 0.4228]\n",
      "\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(env.nS):\n",
    "    print('{}\\n{}\\n'.format(np.round(every_visit_Q[i], 4), np.round(first_visit_Q[i], 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397abe50",
   "metadata": {},
   "source": [
    "# Monte Calor Control with ${\\varepsilon}$-Greedy\n",
    "\n",
    "## TODO\n",
    "- 그림과 함께 설명 추가\n",
    "- Dynamic Programming의 Policy iteration과 어떤 차이가 있는지\n",
    "- epsilon greedy 방법 설명 (탐험)\n",
    "\n",
    "설명 추가\n",
    "- Q function 사용\n",
    "- epsilon greedy\n",
    "- value iteration 차용\n",
    "- GILE: Greedy in the Limit Infinite Exploration"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
